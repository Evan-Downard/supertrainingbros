{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49aa7183",
   "metadata": {},
   "source": [
    "# Super Training Brothers Assignment 2 (11/29/2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92839c8",
   "metadata": {},
   "source": [
    "Team Members: \n",
    "Gabriel Simiyu, Evan Downard, Jaden Hicks, and Gabriel Koeller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3c846",
   "metadata": {},
   "source": [
    "## Deep Learning Problem\n",
    "\n",
    "   People who play games usually aim for the highest score. However, the best methods for getting a high score are not always known. By using reinforcement learning, we can teach a computer to compute the best possible actions to gain points. This can then be displayed to players in video form to teach them how to improve their skills, increasing the competitiveness of the arcade version of Mario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8559db2-76f7-4c37-96b0-e4ed071ff697",
   "metadata": {},
   "source": [
    "### Generalization of the Problem\n",
    "Furthermore, the general problems presented by video games have many real-world applications. Oftentimes the optimal method for completing a task is not clear. The task may require multiple steps performed in sequence; there may be more or less efficient ways to complete the task; the optimal method may vary significantly between circumstances, making a set algorithm impossible or inefficient. In a video game like Mario Bros, Mario has to navigate changing environments, avoid fireballs that appear in different locations, and gain points by performing a series of actions: knocking the wall under the turtle to flip the turtle, jumping onto the wall, and then knocking the turtle. "
   ]
  },
  {
   "attachments": {
    "mario.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAHJCAIAAACzOyKPAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABfzSURBVHhe7d17cFzXfR9w7mKxeBAPAuAL4kukqBdtPUJJpiQ7tiNZTWXXGbtJp46ceNKkM0kz7WjGM52Mp578kZHrdjqjf9JJ808edhTXnTR1prajpLFdxXEssZJoSZQoiuJD4kPgAwCJN7AAFj3kPVxDIEguePHYBT6f+c3B+e09ywvBFr+6d3cPVgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACwtDLxK1SAbCZXV9sextiXYXxyoDAxEJsyLPQp8rnm2prm2JShODUxNt4bxtiXYU6nWIE/H1h8opQKsrquc9emf9VY1xn7Mrx77tl3uv86NmVY6FPcvPaT29Y9HpsyDI91HTj1J0NjXbEvw5xOsQJ/PrD4RClLL59r6VzzcBjrcms2tX8sjMnjPYNv9A6+kcxLLi1+KJ9rTdrugVfPDbySzAsT/V0XfhzGpC3paPpAe9MHknmaU4SVYX3y4AylU6xrvndt8z3Jg4WJvq4Lz1/5/YSVYX0yH5u4cKr378OYtLOeovTzSdrSKWb95hOlU6yEnw8sOVHK0muu33r/ji+FMfaXvdX15291fSM2l11tcTAwevylo18NY+wvu73zids7Px+b95vTKWZdnJj1FL6fkgX9fmDJZeNXAOCGiFIASEWUAkAqohQAUhGlAJCKKAWAVEQpAKQiSgEgFVEKAKmIUgBIxcaBLL1L+75+vK52TV1Ny8aLm7vGzWbPDbzS3R/3jy0Jyy4uvrxJ7MVNWYcOJPOx8Qunep8r7ddacnHf15Z7k3maU4SVpf1sZyidomP1rtJ+tpf2j30ufFdJWxJWhvXJvDDRf/rC82OTcR/aWU9R+vkkbekUs37zidIpVsLPB5acKKWCXG1z12uY66asC32Ka+wfO6ur7UN7DXM6xQr8+cDic4MXAFJxVUoFmXEnsxxzveO30KeYfrO0HFe76XoNczrFCvz5AAAAAAAAAAAAAAAAAAAAAAAVaPbdjmoaamrqa2JTASZHJydHJmMDwEpS+ZE0e5SuvX/t2gfWxqYCdL/Y3f1Sd2wAWEkqP5Jmj9LORzpDxaYCdP2gK1RsAFhJKj+S/GYYAEhFlAJAKqIUAFIRpQCQiigFgFREKQCkIkoBIBVRCgCpiFIASEWUAkAqohQAUhGlAJCKKAWAVEQpAKQiSgEgFVEKAKmIUgBIRZQCQCqiFABSEaUAkIooBYBURCkApCJKASAVUQoAqYhSAEhFlAJAKqIUAFKpiV/fr3l7c6jYzLep4tTk0OTk6GRxrFhmDRwbGDoxFJ8PwErSeFNjw4aGGblwjZoan8rkMkF8/nwbPDYYKjaXzH6mzkc6Q8Vmvo2dH+v6XlcYY1+G8b7xQl8hNgCsJPnWfG1rbWzKUNdW1/mJzjDGfr51/aArVGwuWYIoHT07evSbR8MYewCYP/Xr63d8bkcYYz/froxSr5UCQCqiFABSEaUAkIooBYBURCkApCJKASAVUQoAqYhSAEhFlAJAKqIUAFIRpQCQSmbdQ+vidJrWW1tbbmuJzXyzBy8AC2eh9+DtP9Tf93ZfbC7J7H5qd5wuFlEKwMJZ6Ci9khu8AJCKKAWAVEQpAKQiSgEgFVEKAKmIUgBIRZQCQCqidN40NubuvX/9wx+9Kakt25rjAQCWNVE6b5pb8z/32NZf+KWdSd2+qz0eAGBZE6UAkIqNA9NqbMzddmd7w+pcribbsiYfxuTxoaHxUMl8ZGji0Ju9w8MTSQvAwln8jQNrOh/pjNPFMjE0cf7182GMfQXLZjMNjbV19bl8Xc30WpVZNTk5laxpX1v/C7+48749G9o66n/4/ZOvv9Zz7EhfqB23rvn5T918x672UOs2NB462Ds0GJMVgIWTW51r+2BbGGO/8FyVXktbe/0nPrktjLG/7OW9p1/eeyaZb+hsfOLXdoWxUJg8e3q4UCgmj7e115WeeKZr+Bt/eiCMSQvAwrGdfWXJ12U3b2nesbN1Rl0ZrkE+X7N5608Xz7oGgOVHlM4UErGpuTapxsbabE18fLrpa+rrc6OjE4MD48NDE8VivOsbhIvU8GBSw8Pjxcn4OADLjBu8M923Z8N9ezYm83w+u35jYwjOpC053zt6vncsmQ8NFl7b1z04WGh//93gl/eeeWnv6WQ+XiheuvcrTgEW3OLf4BWlF9+Ce+ud7WFM2tt3td/xgTl8JHSgv/Di86fDmMtdegdvLl7ov/VG78EDvckcgEUjSpdA6X1Dsb9R3lsEUAkWP0q9VgoAqYhSAEhFlAJAKqIUAFKxceDF3QFrcplzZ4dPHh8IFR5paa1LDpVjZHjiwP6ed470HX+n/92j/YUxn3gBWEo2Dlx6n3h8W6hkXixOjY1OTt94IZHP19Tm4wW9N+4CVJTFfwevq9KZdty6JlQyv3B+7G+//c6Lz5/e/0r39MrmMjdtakrWDA2O73/lnK3qASrE4l+Veq30WgpjxRPH+4+8fWFGne+pgr34AVgcNeseXFccLy5mFfoKfW/2TVTML+/MZjP19bm6upp8/mJt2da8oXP1eKEYqq+vcGB/z/DQeDlr4h8HwJLKNeSadzZn89kZ6bNwlWne0RxPvliKheLI2ZEwxn6ptbXXP/pPt5b2zk1+22gyL4xNnum6uHduOWuSFoClFUK0YX1DGGO/8DJfuPvuOF2pmlvyDzy0saU1n7QH3+gNlcxLyllDxXr1zJlQsWEp3LNhQ6jYwLKTOfnkk3EKy9TTL7zw9N69sWEpfHHPni8++GBsYNnxtiMASEWUAkAqohQAUhGlAJCKKAWAVDKv/+ZvxiksU3/w8st/8NJLsWEp/Pb99//2fffFBpadzEe2bIlTWKaO9/cf74tbarAktra2bm1piQ0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMH8y8StQqeobcttvaQ1j0p5+b7Dr1FAyByqBKIVKt6Gz8Ylf2xXGpP3es++GSuZAJcjGrwDADXFVCpWoc9PqjTc1JfO6umyYhzFpe7pHQ4WJO71QIUQpVKJPPL4tVDI/0zX8jT89EMakLR1ypxcqhBu8AJBKTfwKLJaNN62+9fa2zptWX1nh6ODAeBh33Lom1KXlqyYni6Mjk23t9cma8Pj6jRffgnT0cF+oZA2whNzghUWSzWZChckjP781VPLgDD/42+OhwuTjj20JlTyYyVx8Yubyv6zF4lSoMCktBpaWKIVFcs/udffctz5M1q1vWLchfrJlhnNnhs+dHQmT904OhkoeXNNW99FHt4QxaV/dd+7Vl8+GSVgZ1icPAkvIDV5YJHfdu27PhztDiK5uqo0PXSEcCgtCHT3ct/cfuy6G5dmRQmFy1wfXNjXHZ73+SndyaHjo4q1gYMl52xEslExmVU1NplTJ3d0yhcXTn1gsTk1Oxkru7gKVww1eWCgzbsxe477ulUp3eoORkYmjhy6EMWnd14VKI0phoczY8O+GzfhcKVBp3OAFgFREKQCk4h28sFByuWxDY+5C79jp94ZChUeamvPJoesK6w+/dSF54qkTA8eO9I+NTsZjQIXxWiksktLeuVNTU8Vi8thM2ezFDRnCxP66UEVEKSySUpRe3GNh38U9Fq50z+719+xeFyaiFKqI10phsZ07M3zgtZ5Zy6dcoBqJUlhA03daCG2yx0KQHE2UswaoZG7wwkJZ01b3s49sLm3R0HVqKPlN3eHS8+zlq89y1gAVLvOZ22+P0wp2sKfnYHd3bKpQc13dhzo7wxh7VoaWNfkPf2xT65r4v/vrr3Tvf3Xm/41by1gDaQyMjb343nsDhULsq9AdHR23r10bm4qUOfnkk3FawZ5+4YWn9+6NTRW6raPjDx9/PIyxB1gsh3p6fuvZZ8MY+yr0xT17vvjgg7GpSF4rBYBURCkApCJKASAVUQoAqYhSAEil5sk9e6ZWrarweuHUqVDxW65CHQ0Nj+/c2d7QMOOfSymlFrq6h4f/+u23e0bib5KvRg9u3rxn06YZ/1wVVZlP7twZv9kK9nZvb6jYVKGmfP5nNm5szpf7W0EA5stAofCT06cHq/lzpbe2t4eKDQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFUlE7/C3OVzq9e37gpj7C87P/TO+cF3YgOw3IlSblxb0/ZP3PN77U3bY3/ZS0f++OXDfxwbgOUuG7/CDclksrOU/0QDVhJRyty0Nd28fcPHktrc8UA+1xgPAKxUrh6Ym/t2/vr9t/x6bC5dlcbZNC8d/qNQsQFY7kQp17G6fv1d2/5FU/36pG1v2tHefEsyv5regSO9g0djc9ng6Nn97/7F0OjZ2AMsF6KU62hr2v7YvU9d+d6iueodPPZ3r3z5/OCx2AMsF14rBYBUXJVyHflc08a2u8IY+9lsXtf07z5736Z1s6w5dW7w97/18slzg4WJwdPn94cxHgBYLkQpc3PH1vY7tnQk8/7hwotvdQ0MF3bd3PHNL396181rk8cPHu89eLwnmZ84N/D0X7x44uxA0gIsP6KUufnyrzz0H37loWT+5rs9n/+P3wnjjCh96pnnv/LM88l8amrVZLEYRoDlymulzE02m6nNZZPK1WQys/3HWIjO8YliUhOTchRY5mriVyjPx+7ZEiqZn7sw/D9/eOjchZF1axp/6aO3hzF5/HTvULgSvXgr+OoVlvX0jyTrAaqaG7zMze9+4eFQyfzAO92fe+rbB96ZeYM3XJWGa9FkfjVfeeb5py7fBAaoaq5KmZtyrkr/6keH//M39/7vHx8O9fKhMx/Y3rGmqb4mm5leHa0NH/7g5s/+7G2hPnRn58HjvQPDheTpANXFa6XMv/3Hzn3j+28m9d29R/oGx+KBae7avu6JR+9M6lN7drQ25eMBgGojSgEgFa+VMjfTXyvtGxr70f5TYWxdXfeRuzaFMXn8977+41DJfMahWZX+nNgDVBVRytxMj9Kr+R//92Co2FzFWyd6Q8UGoJqJUuamnCidvPgO3mJsruIrz7xQ2sYBoKp5rZT5V5PN5HM1166wJq4GqHL+OmNu/uXP3REqmTc35u+/bWMYk/baBoYLLx06XfrEyyuHz4ZK5jMOAVQXUcrchMvJ3OULyju3dfzZlz4VxqS9tjff7fnVr343jEn7O7+8J1Qyn3EIoLrYooG5mZq6uJNRUiNjE4dOnv+b/3fsxYNdd2xtb2mc5W26p7oH/tM39n79/7zxnReOvnbk3NDoePLcj92z5dHd23IhmGuyvf0jf/kPF7d6iM8BqCpeK+XG9Q2NffeFI3/+vQPfeWH2fRiC8Hg4GtaElT7uAixLohQAUvFaKXPz8E3ND21qis1la1bXf+TuzWtm24fhwtDYj147eWFoNPaX3b1j/d071iXzI92Dv/Ot/Ue6h5IWoLqIUubmiTvXfn5X/A0w8+V4/9hX974XxtgDVBU3eAEgFVelXEdHQ+7Tt7SFMWm3t9Zvb73Whrpl+vF7A8+fGkzmQ+OTr3cPD41fZ4MkgMokSrmOrS11X9pzUxhjP83IRPHw+dEwNuSyO9vqwxgPzKa0OGmfO9H/9yf6kzlAVROlXMc1ovRE/9h/ebHrxMDYlua6f/9A55bZ1pSUFiftZHHV5NRUMgeoal4r5caFJBwvThUmp8J43VQsLU5KjgLLhihlaWxqyn+osympu9Y1XvvmMEAl8/cXS+Mjm5u/tGdTUv/67vVrL7+tCaDqeK2U61hdm/3g2sbVtbNt11zTvqrxk2EMRz+4tmH2NZedHTj9zX3PhDFpH7ntn4RK5jMOAVQXUcqNa2va/ti9T7U3bY/9NfUOHvu7V758fvBY0t6/8zdCJfMZhwCqixu8AJCKq1JuXDlXpUOj5944/pdDY+dyNQ3tTTvCmDy+tmVnR/OtydxVKVDVRCnXUZtrXNdye21NY+ynaWrY8DPbfzWMsZ/N4MiZnxz7szBeY7EoBaqaKOU62ppufuTu321bfXPsp8lkMtlMPoyxn83U1FRxqhDGaywWpUBV81op15WpydblauqvrPD4tXM0CAuSp5ezGKAa+auN65jT23QTfUMn+oZPxKYMAyOnf3LsmcERH4YBqpIo5TpuIEr3HfnavqNfi00ZpqaKxeL41MW9BQGqjyjlOmZE6bEzP3zn7A+T+dV097/dM/B2bACWO1HKdcyI0pcO/1GoZA5A4G1HAJCKKAWAVK61/zgEuZr65oYNI4ULfcMnQ713fl/vwJF4DACvlXJdlz4Ymi/dwChOjReLE8kcAAAAAAAAqlfm6ccei1OYzenBwa/v3x/G2MPKs7Gp6Qt33RXG2MP7ZU4++WScwmwO9fT81rPPhjH2sPLc1tHxh48/HsbYw/v5XCkApCJKASAVUQoAqYhSAEhFlAJAKplnPvOZOIXZnOzv/68vvnhyYCD2sPJsbm7+tw88sLmlJfbwfpmmfD5OYTbFqanRiYkwxh5WnmwmU5/LhTH2AAAAAAAAAAAAAAAsHe/thnnw8d0bPr57fWyqzXP7zj6370xsgLkTpTAP/s0/vzVUbKrNf/tfb4eKDTB3Ng4EgFREKQCkIkoBIBVRCgCpiFIASEWUAkAqohQAUhGlAJCKKAWAVEQpAKQiSgEgFVEKAKmIUgBIxW+GgXmwa3trqNhUmwPH+kLFBgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACWg8yWf7YlThfL+MB4z76eMMYeAOZPbXNtx+6OMMZ+4WV2P7U7ThfL6NnRo988GsbYA8D8qV9fv+NzO8IY+4WXjV8BgBsiSgEgFVEKAKmIUgBIRZQCQCqiFABSEaUAkIooBYBURCkApCJKASAVUQoAqYhSAEhFlAJAKqIUAFIRpQCQiigFgFREKQCkIkoBIBVRCgCpiFIASEWUAkAqohQAUhGlAJCKKAWAVEQpAKSSWf/w+jhdLBPDE/2H+sMYewCYP7nGXMttLWGM/cLL3NHeEKcAMJvukfHuEdc/V5X52id3xikAzOav3u791tu9seEKme/+4h1xCgCzOdgz8mbvSGy4gigFgFS8gxcAUhGlAJCKKAWAVEQpAKSS+eyt7XF6o3ZvWB0qNgCwwmTi1xSeuHPt53etjQ0ArDBu8AJAKpnfuCvtHrx3djSEig0ArDC2aID5lG2pr2mti00ZpsYnJ3tGwhj7MpRzism+sWL/aGyABSZKYT41Pryt8cNbY1OGye7h/m+/GcbYl6GcUwz/4/HhH78bG2CBea0U5lOmPlezpqH8yrbUZbJz+9ewnFOENXE1sPBEKcyDkIi1m1tD1bT89Nbr1Hhx4szg+Mm+GTXZPxZXhFzM1eQ2NiXPDRX+nHhgmkxtNrfhp2umnwKoBG7wwjwo3XTN1teWrggv37wdStqSxg9vC+uT+dTk1NRwYWqymLSz3pitWdvY8uk7w5i0009xNYPfPzL0gyOxARaYq1KYB6WbrtNDbqpYLPaPTV4YnVHFab9COVOTyTbXXfvG7NTw+Oirp4dfOJHU4HNHB/7mUKjC4Z64AlhSohQqXXF4fGTfqeF/eGdGFd69EFcAS0qUAkAqohQAUhGlAJCKKAWAVEQpAKQiSgEgFVEKAKnY7QjmQe22NbVb14RJ3c6O/M6O5MHi8PjYm2fDmLQl+a1rwvpkXhwsjL7SNTkYtxIcP35h/IpPi2ab8vX3dmabZu4XOP3PmSH8IYXjM/+c4uBYOFc4Y+yBeSJKYT6tfuSWpkdviU0ZJs4O9v3318IY+9nk1je1/vLdYYz9jSrnXMANcIMXAFJxVQrzqXbbmvy2ttiUoThUmPUm8HTZxtq6O9dnV+djf6PKORdwA0QpAKTiBi8ApCJKASAVUQoAqYhSAEhFlAJAKqIUAFIRpQCQiigFgFREKQCkIkoBIBVRCgCpiFIASEWUAkAqohQAUhGlAJCKKAWAVEQpAKQiSgEgFVEKAKmIUgBIRZQCQCqiFABSEaUAkErm9x+9OU4BgLnLbGrKxykAAAAAAAAAAAAAAAAAAAAAAAAAAAAsI6tW/X8y07sqfiWDXQAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "c6a2607d",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"attachment:mario.png\" width=\"350\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14754a",
   "metadata": {},
   "source": [
    "Likewise, a robot interacting with the physical world will constantly experience novel environments, with changing obstacles, and complex sequential tasks. The complexity of the task combined with the variance in the environment makes finding the optimal algorithm difficult. Reinforcement learning addresses these problems by constructing algorithms to find the best algorithm given a unique and complex situation. Refining this process, even for a fun and simpler task like designing a game agent, can advance progress for analogous real-world situations, furthering our ability to utilize computers to efficiently solve complex and novel problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c94b3",
   "metadata": {},
   "source": [
    "## Overview of Past and Current Solution Ideas\n",
    "\n",
    "### Past Solutions\n",
    "\n",
    "#### Input Frameworks\n",
    "One past solution assisted gameplay with tools to make inputs on specific frames, allowing inputs to be made at a speed faster than a human. The tools require human knowledge to make the inputs and only remove the human error after the inputs have been determined.\n",
    "\n",
    "Alternatively, reinforcement learning has been used to help a computer to learn how to play the game, rather than assist a human. This latter option aligns more with our goals. Reinforcement learning uses a variety of techniques. A common strategy involves modeling the game as a Markov-Decision Process and trying to learn the optimal Q function, as described below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abaed28-28d5-4f0b-9aa6-abc6603a33ad",
   "metadata": {},
   "source": [
    "#### Markov Decision Processes\n",
    "\n",
    "Markov Decision Processes allow us to explore all possible options within the action space to learn which actions lead to a higher score.\n",
    "\n",
    "MDPs are defined by the tuple $(\\mathcal{S}, \\mathcal{A}, T, r)$.\n",
    "* $\\mathcal{S}$ is set of all game state \n",
    "* $\\mathcal{A}$ is the set of all actions.\n",
    "* $T(s',s,a)$ is the transition function, which returns the probability of a new state $s'$ given the current state $s$ and an action $a$\n",
    "* $r(s,a)$ is the reward function, which returns the reward for taking action $a$ at state $s$\n",
    "\n",
    "With an MDP, we use a reward system instead of a label of “good” and “bad” for each action, and an algorithm called Q-learning trains the agent to maximize rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23166f04-31b0-4bc4-8fec-ed802c692b47",
   "metadata": {},
   "source": [
    "#### Q-Learning\n",
    "\n",
    "Q-learning attempts to learn the value of taking each action at each state. The action-value function $Q^{\\pi}(s,a)$ returns the value of starting a trajectory by taking action $a$ at state $s$, and then using policy $\\pi$ to decide subsequent actions. The function returns the expected value of the total reward obtained from the resulting trajectory. By learning the Q-function, the agent learns what actions can produce the highest rewards at each state.\n",
    "\n",
    "To learn the Q-function, the agent begins by randomly selecting actions. When an action produces a reward, the value of that state/action combination is increased. At each state, the highest value action is most likely to be chosen. Thus, the more an action produces rewards, the more valuable it becomes, and the more likely it will be chosen. Nevertheless, the agent explores less valuable actions sometimes. This helps prevent settling on less optimal solutions. Thus, over many iterations of the game, the agent learns which actions maximize rewards and thus learns to play the game.\n",
    "\n",
    "Q-learning is a form of unsupervised learning. No previous data showing good and bad actions is required to train the model. The model learns the best actions on its own, without initial knowledge of what actions are good or bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d97ff-0c48-4130-a8a0-3b409fea09fd",
   "metadata": {},
   "source": [
    "#### Double Deep Q-Learning\n",
    "\n",
    "Double deep Q-learning (DDQL) attempts to optimize the Q-learning algorithm and prevent overestimation of action values (Hasset, 2015). Double DQN is \"double\" because it involves two Q-function approximations. It is \"deep\" because it uses a multi-layered neural network mapping from *n* states to *m* actions to learn the Q-function rather than an $n \\times m$ table. The local network determines which actions to take, while the target network computes the return of the actions, and its parameters are only updated every several time steps.\n",
    "\n",
    "DDQL has been used for the Super Mario Bros environment, which yeilded similar results to the actor-critic models A3C and TD3 (Schejbal, 2022). Thus, we will focus on the DDQL model, utilizing  Schejbal's helpful findings regarding the best training methodology and hyperparemeter values. Suggestions we used include: using a batch size of 32, learning rate of 0.00025, discount factor of 0.99, and replay buffer size of 500; focusing less on the decay schedule (has low effect on learning); syncing the target network every 1,000 steps; using two convolutional layers rather than three to boost performance without losing accuracy; using the full game state rather of a reduced $84 \\times 84$ image.\n",
    "\n",
    "We will apply the DDQL model to the Mario Bros environment, which differs from the Super Mario Bros environment. In the Super Mario Bros environment, Mario moves right along a path, earning points and avoiding death to complete the level. In the Mario Bros environment, Mario must move vertically up platforms, avoiding fireballs, and knocking turtles to earn points.  Schejbal found that agents learned better for the Super Mario Bros environment when the environment was set to always move right, not allowing the agent to stop or move left. This simplification does not work for the Mario Bros environment, making learning more difficult.\n",
    "\n",
    "Additional Citations:\n",
    "* F. Lina, *Deep Q Network: Combining Deep & Reinforcement Learning*, Aug 2021, https://towardsdatascience.com/deep-q-network-combining-deep-reinforcement-learning-a5616bcfc207"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c64da-a340-4fb8-83dc-5159b5f8e531",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "Along with a DDQN, we use experience replay, which improves learning by keeping a buffer of transitions and randomly sampling this buffer during each training step (Lin, 1992). This allows each training batch to have a more representative sample of transitions, rather than the most recent set.\n",
    "\n",
    "One idea that we would test in the future is Prioritized Experience Replay (Schaul, 2016), which attempts to prioritize the most important transitions when sampling the buffer, rather than sampling them uniformly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bf19a-41a2-4b1b-bbb6-e205c20c15a6",
   "metadata": {},
   "source": [
    "### Environment Pre-Processing\n",
    "\n",
    "Finally, we followed standard technique by simplifying the environment before sending it to the network to reduce training time (Mnih, 2013). Pre-processing technique we used included: (1) reducing the observations from $210 \\times 160$ rgb images to $84 \\times 84$ grayscale images; (2) dividing color values by 255 to normalize them between 0 and 1; and (3) choosing an action and then skipping a few frames to observe the effect of the action.\n",
    "\n",
    "However, (Schejbal, 2022) found that allowing the network to train on the original $210 \\times 160$ images significantly improved performance, so we tested the model with and without reducing the image size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51608eea",
   "metadata": {},
   "source": [
    "## New Solution Ideas\n",
    "\n",
    "We explored the following optimizations of the DDQL model to improve our game agent's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4ecf9-b7ce-491b-8b7f-bfaf140487c5",
   "metadata": {},
   "source": [
    "### Death Penalty\n",
    "\n",
    "In early testing, we tried adapting the reward function to help the agent learn. Firstly, we normalized the reward by dividing it by 800, the base reward value for the game. Secondly, we tried penalizing the agent for dying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3fd01-9f55-4a64-9c39-9c0edce03ffc",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Optimization\n",
    "\n",
    "We experimented with hyper-paramater values to improve the learning of our DQN.\n",
    "* Learning rate\n",
    "* Batch size\n",
    "* Steps until syncing the target network to the local network\n",
    "* Epsilon decay schedule and minimum value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e682ce-42ad-45c3-aa73-963258dc7fc6",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "We made the model more or less complex by:\n",
    "* Adding/removing convolutional layers\n",
    "* Adjusting the number, size, and stride of filters\n",
    "* Using Max Pooling or not\n",
    "* Adding/removing fully-connected layers\n",
    "* Adjusting the number of fully-connected nuerons\n",
    "* Using the GeLU vs. ReLU activation functions\n",
    "* Using the Huber loss vs. the MSE\n",
    "\n",
    "### Human player imitation\n",
    "\n",
    "* While not implemented in our current model further trials could be run with new models based on mimicing human playthroughs (Lee, 2014)(Ortega, 2013) of Mario. Though significant changes in environment would be required as most player data is for the NES and SNES versions of Mario as opposed to Atari."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56170e4c-9829-4e55-a62f-eab09f239de4",
   "metadata": {},
   "source": [
    "\n",
    "#### Additional Citations\n",
    "* M. Volodymyr et al., *Asynchronous Methods for Deep Reinforcement Learning*, Proceedings of Machine Learning Research. June 2016, pp. 1928–1937, https://proceedings.mlr.press/v48/mniha16.html\n",
    "\n",
    "* Bellemare et al., *Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents*, Mar 2018, https://jair.org/index.php/jair/article/view/11182\n",
    "\n",
    "* Liao et al., *CS229 Final Report: Reinforcement Learning to Play Mario* https://cs229.stanford.edu/proj2012/LiaoYiYang-RLtoPlayMario.pdf\n",
    "\n",
    "* S. Klein, *CS229 Final Report: Deep Q-Learning to Play Mario*, https://cs229.stanford.edu/proj2016/report/klein-autonomousmariowithdeepreinforcementlearning-report.pdf\n",
    "\n",
    "* As Grebenisan, *Play Super Mario Bros with a Double Deep Q-Network*, 2020, https://blog.paperspace.com/building-double-deep-q-network-super-mario-bros/\n",
    "\n",
    "* M. Comi, *How to teach AI to play Games: Deep Reinforcement Learning*, Nov 2018, https://towardsdatascience.com/how-to-teach-an-ai-to-play-games-deep-reinforcement-learning-28f9b920440a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546e3fc",
   "metadata": {},
   "source": [
    "## Hardware, Software, and Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71081ebb",
   "metadata": {},
   "source": [
    "#### Hardware Needs\n",
    "\n",
    "   * Reinforcement learning proved more hardware intensive than we anticipated\n",
    "   * Rented machine on Paperspace\n",
    "       * Used A4000 machine with 45GiB RAM, 8 CPUs, and 16 GiB GPU for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46870f-8997-4188-bdaa-06cc05560e86",
   "metadata": {},
   "source": [
    "#### Software Needs *(Same)*\n",
    "\n",
    "   * Python 3.11, TensorFlow 2.14, Keras, Numpy\n",
    "   * JupyterLab, JupyterNotebook\n",
    "   * Gymnasium - 0.29.1\n",
    "   * Ale-py - 0.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4527804-5f85-4009-9935-5b54b6d88203",
   "metadata": {},
   "source": [
    "#### Data Needs\n",
    "\n",
    "   * Game states are obtained through the Gymnasium environment\n",
    "   * Includes\n",
    "        * Observation space\n",
    "        * Action space\n",
    "        * Current state\n",
    "        * Reward and next state\n",
    "        \n",
    "   * Examples of human play for potential future implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625809c1-2a2b-4036-b072-640b8eaface9",
   "metadata": {},
   "source": [
    "## Control Flow for Software System\n",
    "\n",
    "Give a diagram of the control and data flow for the software system\n",
    "List significant software tasks, experiments, evaluation, testing, and results accomplished since the last assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ba17c",
   "metadata": {},
   "source": [
    "![UML_class.png](https://cdn.discordapp.com/attachments/1164575450392166463/1179456411894304868/UML_class_2.png?ex=6579d961&is=65676461&hm=5e8af31c93d4c6efeb31a27b75f78f47c93c1b94e52c5e0ca09feb06019e7c79&)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9605dd68",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "   * Training for 1000 episodes\n",
    "   * Game State downsampled to 84x84 and grayscaled to allow for faster training\n",
    "   * ~25 episodes an hour with high end desktop (RTX 3080, Ryzen 7 5000 series)\n",
    "   * ~300 episodes an hour, machine on Paperspace\n",
    "   \n",
    "#### Results \n",
    "\n",
    "   * Early training led to Mario running in a straight line to the left\n",
    "   * Reward function had to be reimplemented, negative penalties increased\n",
    "   * The model (Mario) learned to avoid hazards but would not gain points\n",
    "   * Reduced death penalty to encourage greater exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86645a",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "| Team Member | Contributions |\n",
    "| :- | :- |\n",
    "| Gabe Simiyu | Model training, Further article research  |\n",
    "| Evan Downard | Model training, Data retrieval for human play model |\n",
    "| Jaden Hicks | Further optimize existing model, Model training |\n",
    "| Gabe Koeller | Research into models based on human play |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9471bd70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Team Report\n",
    "\n",
    "| Team Member | Contributions |\n",
    "| :- | :- |\n",
    "| Gabe Simiyu | Model training, Article Research |\n",
    "| Evan Downard | Model training, Notebook presentation setup, Article research |\n",
    "| Jaden Hicks | Model training, Resource utilization, Primary model development |\n",
    "| Gabe Koeller | Model training, Investigation/Development of alternative model |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77559479",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Full Reference List (with commentary on select articles)\n",
    "* Hasselt et al., *Deep Reinforcement Learning with Double Q-learning*, Sep 2015, https://doi.org/10.48550/arXiv.1509.06461\n",
    "\n",
    "* F. Lina, *Deep Q Network: Combining Deep & Reinforcement Learning*, Aug 2021, https://towardsdatascience.com/deep-q-network-combining-deep-reinforcement-learning-a5616bcfc207\n",
    "\n",
    "* Bc. Schejbal, *Deep Reinforcement Learning for Super Mario Bros*, Feb 2022, Masters Thesis for CTU in Prague, https://dspace.cvut.cz/bitstream/handle/10467/101068/F8-DP-2022-Schejbal-Ondrej-thesis.pdf?sequence=-1&isAllowed=y\n",
    "\n",
    "* Lin, LJ. Self-improving reactive agents based on reinforcement learning, planning and teaching. Mach Learn 8, 293–321 (1992). https://doi.org/10.1007/BF00992699\n",
    "\n",
    "* Schaul, T. et al., *Prioritized Experience Replay*, 2016, https://doi.org/10.48550/arXiv.1511.05952\n",
    "\n",
    "* Mnih, V., et al., *Playing Atari with Deep Reinforcement Learning*, 2013, https://doi.org/10.48550/arXiv.1312.5602\n",
    "\n",
    "* Mnih, V. et al., *Asynchronous Methods for Deep Reinforcement Learning*, Proceedings of Machine Learning Research. June 2016, pp. 1928–1937, https://proceedings.mlr.press/v48/mniha16.html\n",
    "\n",
    "* T. Shu et al, *Experience-Driven PCG via Reinforcement Learning: A Super Mario Bros Study*, Jul 2021,\n",
    "https://arxiv.org/pdf/2106.15877.pdf\n",
    "\n",
    "* G. Lee et al, *Learning a Super Mario controller from examples of human play*, Sep 2014,\n",
    "https://ieeexplore.ieee.org/abstract/document/6900246\n",
    "\n",
    "* J. Ortega et al, *Imitating human play styles in Super Mario Bros* Apr 2013,\n",
    "https://www.um.edu.mt/library/oar/bitstream/123456789/29589/1/Imitating_human_playing_styles_in_Super_Mario_Bros.pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Bellemare et al., *Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents*, Mar 2018, https://jair.org/index.php/jair/article/view/11182\n",
    "\n",
    "In the above article Machado et al introduces the Arcade Learning Environment (ALE) which is an evaluation platform that both allows for the building and testing of AI agents with general competency across several Atari 2600 games. This article was particularly interesting as it explored numerous evaluation methods as well as best strategies for training AI agents specifically for Atari 2600 environments.\n",
    "\n",
    "* Liao et al., *CS229 Final Report: Reinforcement Learning to Play Mario* https://cs229.stanford.edu/proj2012/LiaoYiYang-RLtoPlayMario.pdf\n",
    "\n",
    "In their paper Yizheng Liao et al outlines their approach to designing an AI agent to traverse the Super Mario Bros. environment. Liao et al achieved a 90% win rate implementing Q-learning. This article was particularly interesting as they outlined exactly how their reward system was structured and how they built a markov model of the mario environment.\n",
    "\n",
    "* S. Klein, *CS229 Final Report: Deep Q-Learning to Play Mario*, https://cs229.stanford.edu/proj2016/report/klein-autonomousmariowithdeepreinforcementlearning-report.pdf\n",
    "\n",
    "-Demonstration of down sampling to black and white\n",
    "-Train a Mario controller agent, which can learn from the game raw pixel data and in-game score\n",
    "\n",
    "* As Grebenisan, *Play Super Mario Bros with a Double Deep Q-Network*, 2020, https://blog.paperspace.com/building-double-deep-q-network-super-mario-bros/\n",
    "\n",
    "This article by Andrew Grebenisan, served as an introduction into how exactly we can begin approaching building an AI agent to get through the super mario bros environment. He begins by overviewing reinforcement learning, deep learning, and double deep Q-learning as well as their shortcomings such as \n",
    "\n",
    "* M. Comi, *How to teach AI to play Games: Deep Reinforcement Learning*, Nov 2018, https://towardsdatascience.com/how-to-teach-an-ai-to-play-games-deep-reinforcement-learning-28f9b920440a\n",
    "\n",
    "    In this article Mauro Comi, a PhD student in machine learning at the University of Bristol, shares how to build and train a simple AI agent that is able to play Snake. In his example, he implements deep reinforcement learning through a deep Q-learning algorithm using Bayesian Optimization to optimize the artificial neural network. This was particularly interesting as they implemented deep Q-learning which allows for more ambiguous decision making from the model. \n",
    "\n",
    "\n",
    "### Other Material\n",
    "\n",
    "* Atari Games Manual: https://atariage.com/manual_html_page.php?SoftwareLabelID=286\n",
    "* Gymnasium API Documentation: https://gymnasium.farama.org/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3f00e",
   "metadata": {},
   "source": [
    "## Software\n",
    "\n",
    "### Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
